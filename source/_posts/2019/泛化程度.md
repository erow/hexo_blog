---
title: 泛化程度
date: 2019-05-17 16:29:26
tags: [ML]
categories: [论文]
---

# 术语 
可能一些词语使用的不太恰当,一时也想不出好的词,可能存在冒用.  

- 有效区间: 激活函数导数较大的区域.如果导数很小,那么梯度无法传播,权值得不到调整,因此认为是无效的(sigmoid的有效区域为(-3,3),tanh为(-1,1) 这类曲线的界限比较模糊. Hardtanh,或ReLU这类分段线性的函数则是确定的)  
- 泛化程度:如果较少的数据用来学习,那么泛化能力必然低;反之亦然.类似,训练样本在神经有效区域内的数目,决定了学习的空间.数目越多则泛化程度越高. 用t=有效区域内的样本/样本总数 表示.  
- 宽度: 有效区间长度/2  
- 中心: 有效区间的中心.



前面尝试在激活函数前,通过一系数k使得神经元在训练集中被激活的比例达到t.  

在神经网络激活函数前增加一系数k:
$$g(k*Z),Z = w x +b$$
假设g为sigmoid.k越大,则其有效区域越小;反之越大.
$$ width=\frac{1}{k*w}, center = -b/w$$
k可以改变有效区域长度,而不改变其中心.

由于系数k直接直接根据激活占比t,只能改变宽度.这样直接固定了神经元的泛化程度,实际上减少了神经元的差异性.
![](/blog_images/2019-05-17-20-05-19.png)
# [实验](https://www.kaggle.com/clouderow/nn-nb1?scriptVersionId=14292401)

生成的数据主要集中在原点附近,同时引入一点干扰以确认不会过拟合.  
![](/blog_images/2019-05-17-18-20-50.png)

网络结构如下:

    mid =50
    model = nn.Sequential(
        nn.Linear(1,mid),
        nn.Tanh(), # 有效趋于为-1~1,比sigmoid更好判断.
        nn.Linear(mid,1)
    )

## 结果一: center和loss
经过16001轮训练后,我发现其50个神经元的中心移动轨迹如下:  
![](/blog_images/2019-05-17-18-25-20.png)  
横坐标是中心的位置,纵坐标是迭代的次数,同一颜色是同一神经元.(从下往上看就是一个神经元的中心变化情况)

可以发现0-10和60-100之间出现了2次快速移动的情况.同时观察loss的变化,可以发现在神经元移动的同时,loss也在快速下降:  
![](/blog_images/2019-05-17-18-30-20.png)

## 结果二: center和width
观察中心点和宽度的关系,图左为未训练的关系图,图右为训练后:    
![初始状态](/blog_images/2019-05-17-18-36-47.png)![训练后](/blog_images/2019-05-17-18-35-01.png)  
可以明显观察到,原点附近的神经元宽度更小.

## 结果三: shift
$$ width=\frac{1}{w}=\frac{center}{-b}=, center = -b/w$$
这表明神经网络可以在原点附近的刻画细致的形状,而远离原点趋于线性表达.因此对原有数据进行shift.  
神经元未在0.5附近聚集,且其宽度也较大,因此拟合程度欠佳.  
![](/blog_images/2019-05-17-19-01-26.png)![](/blog_images/2019-05-17-19-02-00.png)



## 结论
训练神经元可以分为2个阶段,移动和塑形.移动阶段,loss能快速地下降,这也是调整网络形状的黄金时期.而在塑性阶段,由于只改变后面一层网络的权值所以修改幅度有限,loss下降缓慢,趋于收敛.

更小的宽度有利于刻画更加细致的形状.

理想的center-width为 神经元聚集在数据密集的地方,且越密集width越小. 

# 问题
以上讨论说明了数据处理的重要性.最好将其处理的到(0,1). 但是如果真实数据是双峰的呢?

能否提出一种权值初始化的方案, 使得网络一开始就能快速学习?

用正态分布初始化权值,聚集在0.5附近;10001轮以后  
![](/blog_images/2019-05-17-20-34-21.png)![](/blog_images/2019-05-17-20-36-09.png)

分段线性函数(Hardtanh,ReLU等)区别挺大的.移动不明显.