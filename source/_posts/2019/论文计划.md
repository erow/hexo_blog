---
title: 论文计划
date: 2019-05-07 14:05:29
tags: ［翻译,资料］
categories: [ML]
---
我想写一篇关于修改数据分布提升神经网络性能的文章.我认为神经网络是通过选择数据中的部分关键数据,不断进行筛选,汇总,最后达到极高的性能.

http://ssci2019.org/

# 资料:  
[拟合能力](https://zhuanlan.zhihu.com/p/44406909)
[Batch normalization](https://arxiv.org/abs/1502.03167)  
[BN 为什么效果好?](https://www.zhihu.com/question/38102762)  
[Efficiency BP](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)  
[数据处理:白化、去均值、归一化、PCA](https://blog.csdn.net/zkp_987/article/details/78684855)  
Covariate Shift : Covariate Shift Adaptation by Importance Weighted Cross Validation


# 技术 


# 实验

## sigmoid, d = 0.1
![信息留存百分比](/blog_images/2019-05-08-21-22-09.png)![缩放比k](/blog_images/2019-05-08-21-24-45.png)

## Relu
ReLU是非饱和的非线性函数,因此无法通过缩放区间来增加或者减少数据容量.只能通过移动bias来修改容量.由于很难判断那个方向是增加或者减少数据容量,所以通过试验法.  
![net1为ReLU,net1在ReLU前增加偏置](/blog_images/2019-05-08-21-59-35.png)  
![上为占比,下为bias,有16个神经元失效(占比为0)](/blog_images/2019-05-08-22-08-59.png)

## tanh 
$$ tanh(k x)/k$$
在缩放区间的同时,尽量保持原来部分的形状.

使用MNIST训练,  下面是采用非缩放情况下的tanh.  
Test set: Average loss: 0.1510, Accuracy: 9567/10000 (96%)  
![losses](/blog_images/2019-05-10-13-12-17.png)  
![t,平均占比在0.9以上](/blog_images/2019-05-10-13-14-47.png)  


d=0.9缩放  
Test set: Average loss: 0.1180, Accuracy: 9659/10000 (97%)  
![losses](/blog_images/2019-05-10-13-24-41.png)  
![t,平均0.88](/blog_images/2019-05-10-13-27-49.png)
![k,收敛慢](/blog_images/2019-05-10-13-30-52.png)