---
title: 理解神经元
date: 2019-04-29 9:26:01
tags: ［构想］
categories: [ML]
---

神经元计算公式为:
$$a=g(w_i^T x)$$
g为sigmoid函数,其特点是在左侧接近0,在右侧接近1,其导数也具备类似性质.  
![](/blog_images/2019-04-29-09-43-05.png)

再看BP算法中梯度的计算.  
![简化模型](/blog_images/NN.jpg)  
 $$\delta^l=\frac{∂z^{l+1}}{∂z^{l}}=w^{k^T}z^{l+1}.*a^l.∗ (1−a^l)$$
 $$\frac{∂J}{∂w^{l}}=\delta^{l+1}a^{l^T}$$

与第l层神经元$a^l$有关的权值有$w^{l}$,$w^{l+1}$.$a^l$越大,$w^{l+1}$的梯度越大;而$a^l$太大或者太小,$w^{l}$的梯度都越小.总的来说就是$a^l$接近0,那么对前后的权值都没有影响.如果接近0.5,那么前后的权值都能得到调整.如果接近1,那么只有后面的权值能够得到调整.

# 前后weight
前部weight决定了该神经元是否被激活,而后部weight决定了最后输出的形状.用$w_1$,$w_2$表示.
如果神经元的值为0或者1,那么它不能产生有效的形状,因此是无效的.
$$-4<w_1 x+b<4$$
$$w_1 x+b=0$$
所以神经元的有效区间长度为$8/w_1$,区间为$[(-b-4)/w_1,(-b+4)/w_1]$

这样就可以对神经元进行可视化.实际上最后的曲线是多个神经元叠加的效果,而这里显示的是单个神经元的效果.  
![](/blog_images/2019-04-29-12-10-50.png)

$$ len=8/w, center = -b/w$$

# 调整神经元形状
$$ len = -8*center/b$$
这个算式说明,中心远离原点的神经元其有效区间长度也比较大.也就是说在远处神经元无法精确拟合一段函数.  
对原来的算式进行一点修改,  
$$a=g(k*(w_i^T x))$$
得到:  
$$ len=\frac{8}{k*w}, center = -b/w$$
这样就在不改变中心点位置的情况下调整有效区间的长度,因而可以提高神经网络的精细程度.

如图为k=0.3时的图形,此时函数可以看作是多个神经元叠加的效果,因此更加光滑.  
![](/blog_images/2019-04-29-13-02-48.png)  
因此如果神经元数量过多,而数据量太少就会导致的**过拟合**.其实就是部分神经元没有得到训练,因为他们的有效区间很窄.通过正则化,减少权值大小,扩大有效区域长度.但是如果神经元的有效区域太大,最后的形状就是所有神经元共同决定的,这样也就很难习得精确的形状.

![40轮](/blog_images/2019-04-29-13-15-06.png)
![400轮](/blog_images/2019-04-29-13-30-10.png)  
60个神经元拟合sin函数,k=1

那么反过来,提高weight减少有效区长度.这样保证在一段内只有少数神经元得到激活训练,这样可以获得更加精确地拟合.

![400轮](/blog_images/2019-04-29-13-22-01.png)  
60个神经元拟合sin函数,k=10

# 总结

这也解释了神经网络中的很多事
## 为什么要对数据进行处理?
由于目前神经网络初始化的方法导致神经元的中间往往在原点附近,因此远离原点的数据收敛也慢.
## 为什么深度网络难以训练?
除了梯度爆炸\梯度消失等问题外,由于每增加一层网络都会产生一些没有被激活或者是饱和的神经元,到最后一层时,有效区域变得很少.


# 二维数据的理解

同样对于一层神经元可以分为前后两部分,前部的weight和bias选择区域,后部的weight和bias决定形状.我选择一个二元函数来验证这一点:
$$z=\sin \left(x+0.1 y^2\right)$$
![](/blog_images/2019-05-02-17-38-51.png)

使用20个隐层神经元的网络来拟合该函数.  
![](/blog_images/2019-05-02-17-41-58.png)

利用公式 
$$ -3<x*w_1+ y *w_2 + b<3$$
得到有效区域.![第8,10个神经元的有效区域](/blog_images/2019-05-02-17-40-34.png)

然后分别将第8,10个神经元的后部权值置0(抹除该神经元的效果)后与原输出相减,绘制等高图.  
![抹除第8个神经元](/blog_images/2019-05-02-17-49-10.png)
![抹除第10个神经元](/blog_images/2019-05-02-17-50-23.png)
 蓝色为0,黄色越深数值越大.

![](/blog_images/2019-05-02-20-24-32.png)

#总结
因此神经网络可以理解为**选择**与**重构**.一个神经元选择数据集中的部分区域得以激活,然后在这块区域重构出目标函数的形状.