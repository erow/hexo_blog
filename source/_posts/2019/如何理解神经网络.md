---
title: 如何理解神经网络
date: 2019-04-16 16:44:01
tags: ［构想］
categories: [ML]
---

# 假设
机器学习很大程度上就是在建立一个模型F,使得F(X)能够很好地描述Y.即求F,$F(X)=Y$.并且有可以证明神经网络能够以任意精度模拟任意函数.这被称为[万能近似定理（universal approximation theorrm）](https://blog.csdn.net/guoyunfei20/article/details/78288271).听上去挺牛逼的,但实际很普通,因为有许多别的方法可以达到这一效果,最简单地可以使用多项式展开来逼近.

所以问题的关键不是具备万能近似这个能力,而是如何快速有效地达到效果.SVM过慢,多项式展开会特征爆炸也过慢,而深度神经网络可能过拟合因而无效.不同模型在实现近似的时候气作用方式,效果都是不一样的.  
我的假设是**任何一种模型,乃至训练方法,会产生不同的形状.**

## SVM
比如高斯核的SVM会在支持向量附近的空间中产生一块圆形的区域.

## 神经网络
对于线性模型,可以用一个超平面进行划分.而更复杂的模型就很难解释究竟是怎么工作的.一种解释办法是,多层神经网络进行了多次空间的划分和变换.尽管他们都有近似的能力,但是他们对同一问题的学习速度和效果会有不同.

做一个比喻.数据在空间中有特定的分布,这就好像是一个具有某种形状的模板,或者说是一个锁.而学习任务就是要生成一把要钥匙来打开它.而不同的模型和方法就是做钥匙的材料.如果你选择用木头来削出一把钥匙只要手艺好,也不是不可能,但是一定很费力.

# 怎么理解?
基于上面的假设,可以从材料的特点入手.[对抗样本](https://zhuanlan.zhihu.com/p/42667844)是一个很好的方法.
## 神经网络是多孔洞结构
一些实验表明,可以通过修改极少的像素就可以导致分类器出错.这说明在一个分类为1的特征附近也存在其他分类的特征.就好像海绵一样,中间有很多的孔洞通向其他分类.

## 其本身也具备特定的形状
尽管我们是希望神经网络能够形成特定的形状,但是它本身也会产生一些形状.即使用不同的训练集,甚至是不同的网络结构,攻击样本都能取得良好的效果.因为一些攻击样本是独立于特定网络的.

## 有偏好
令神经网络学习某一特征最常用的办法就是增加数据.但是这很有可能是错误的,并不是出现的次数多就会引起关注.一些实验表明神经网络倾向于局部纹理特征,而非全局特征.


# 总结
我认为理解神经网络最好的办法就是了解它的特性,而非权值可视化.通过构造一些特殊的特征,或者对原始图像进行修改然后分析网络的变化.通过对抗样本,消除特定特征等方式使神经网络发生根本性变化(输出不同的分类).那么这种方法就是神经网络所具备的特定.

相关领域: 对抗样本,sensitivity analysis.